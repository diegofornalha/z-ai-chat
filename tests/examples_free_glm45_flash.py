#!/usr/bin/env python3
"""
üÜì EXEMPLOS PR√ÅTICOS COM GLM-4.5-Flash (MODELO GRATUITO)

Este modelo √© 100% GRATUITO e voc√™ pode usar agora mesmo!
Limite: 2 requisi√ß√µes simult√¢neas

Casos de uso inclu√≠dos:
1. Chat simples
2. Tradu√ß√£o de texto
3. Resumo de textos
4. Gera√ß√£o de c√≥digo
5. An√°lise de dados
6. Cria√ß√£o de conte√∫do
7. Conversa√ß√£o com hist√≥rico
8. Streaming (resposta em tempo real)
"""
from dotenv import load_dotenv
from zai import ZaiClient

# Carrega .env
load_dotenv()
client = ZaiClient()

# Modelo GRATUITO
MODEL = "glm-4.5-flash"


def exemplo_1_chat_simples():
    """Exemplo 1: Chat simples e direto"""
    print("=" * 60)
    print("üÜì EXEMPLO 1: Chat Simples")
    print("=" * 60)
    print()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Explique o que √© intelig√™ncia artificial em 2 par√°grafos"}
        ]
    )

    print("üí¨ Resposta:")
    print(response.choices[0].message.content)
    print()
    print(f"üìä Tokens usados (GR√ÅTIS): {response.usage.total_tokens}")
    print()


def exemplo_2_traducao():
    """Exemplo 2: Tradu√ß√£o de texto"""
    print("=" * 60)
    print("üÜì EXEMPLO 2: Tradu√ß√£o de Texto")
    print("=" * 60)
    print()

    texto_original = """
    Good morning! I hope you're having a wonderful day.
    Artificial intelligence is transforming the world.
    """

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": f"Traduza o seguinte texto para portugu√™s:\n\n{texto_original}"
            }
        ]
    )

    print("üìù Texto Original:")
    print(texto_original)
    print()
    print("üåç Tradu√ß√£o:")
    print(response.choices[0].message.content)
    print()


def exemplo_3_resumo():
    """Exemplo 3: Resumir textos longos"""
    print("=" * 60)
    print("üÜì EXEMPLO 3: Resumo de Texto")
    print("=" * 60)
    print()

    texto_longo = """
    Intelig√™ncia artificial (IA) √© um ramo da ci√™ncia da computa√ß√£o que se prop√µe
    a elaborar dispositivos que simulem a capacidade humana de raciocinar, perceber,
    tomar decis√µes e resolver problemas. A IA come√ßou como um campo acad√™mico em 1956
    e tem evolu√≠do drasticamente desde ent√£o. Hoje, machine learning e deep learning
    s√£o subcampos da IA que utilizam redes neurais para processar grandes quantidades
    de dados. Aplica√ß√µes modernas incluem assistentes virtuais, carros aut√¥nomos,
    diagn√≥sticos m√©dicos, e muito mais. A IA est√° transformando ind√∫strias inteiras
    e mudando a forma como vivemos e trabalhamos.
    """

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": f"Resuma o seguinte texto em 3 pontos principais:\n\n{texto_longo}"
            }
        ]
    )

    print("üìÑ Resumo:")
    print(response.choices[0].message.content)
    print()


def exemplo_4_geracao_codigo():
    """Exemplo 4: Gera√ß√£o de c√≥digo"""
    print("=" * 60)
    print("üÜì EXEMPLO 4: Gera√ß√£o de C√≥digo")
    print("=" * 60)
    print()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": "Escreva uma fun√ß√£o Python que verifica se um n√∫mero √© primo"
            }
        ]
    )

    print("üíª C√≥digo Gerado:")
    print(response.choices[0].message.content)
    print()


def exemplo_5_analise_dados():
    """Exemplo 5: An√°lise de dados"""
    print("=" * 60)
    print("üÜì EXEMPLO 5: An√°lise de Dados")
    print("=" * 60)
    print()

    dados = """
    Vendas Q1 2025:
    Janeiro: $50,000
    Fevereiro: $62,000
    Mar√ßo: $58,000
    """

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": f"Analise esses dados de vendas e forne√ßa insights:\n\n{dados}"
            }
        ]
    )

    print("üìä An√°lise:")
    print(response.choices[0].message.content)
    print()


def exemplo_6_criacao_conteudo():
    """Exemplo 6: Cria√ß√£o de conte√∫do"""
    print("=" * 60)
    print("üÜì EXEMPLO 6: Cria√ß√£o de Conte√∫do")
    print("=" * 60)
    print()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": "Escreva 3 ideias de posts para redes sociais sobre tecnologia"
            }
        ]
    )

    print("‚úçÔ∏è Ideias de Conte√∫do:")
    print(response.choices[0].message.content)
    print()


def exemplo_7_conversacao_com_historico():
    """Exemplo 7: Conversa√ß√£o mantendo hist√≥rico"""
    print("=" * 60)
    print("üÜì EXEMPLO 7: Conversa√ß√£o com Hist√≥rico")
    print("=" * 60)
    print()

    # Hist√≥rico de mensagens
    messages = [
        {"role": "user", "content": "Ol√°! Meu nome √© Jo√£o e eu gosto de programa√ß√£o."},
    ]

    # Primeira intera√ß√£o
    response1 = client.chat.completions.create(
        model=MODEL,
        messages=messages
    )
    print("üë§ Voc√™: Ol√°! Meu nome √© Jo√£o e eu gosto de programa√ß√£o.")
    print(f"ü§ñ GLM-4.5-Flash: {response1.choices[0].message.content}")
    print()

    # Adiciona resposta ao hist√≥rico
    messages.append({
        "role": "assistant",
        "content": response1.choices[0].message.content
    })

    # Segunda intera√ß√£o - teste de mem√≥ria
    messages.append({
        "role": "user",
        "content": "Qual √© o meu nome e o que eu gosto?"
    })

    response2 = client.chat.completions.create(
        model=MODEL,
        messages=messages
    )
    print("üë§ Voc√™: Qual √© o meu nome e o que eu gosto?")
    print(f"ü§ñ GLM-4.5-Flash: {response2.choices[0].message.content}")
    print()


def exemplo_8_streaming():
    """Exemplo 8: Streaming (resposta em tempo real)"""
    print("=" * 60)
    print("üÜì EXEMPLO 8: Streaming (Resposta em Tempo Real)")
    print("=" * 60)
    print()

    print("ü§ñ GLM-4.5-Flash (streaming): ", end="", flush=True)

    stream = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Conte uma hist√≥ria curta sobre um rob√¥"}
        ],
        stream=True
    )

    for chunk in stream:
        if chunk.choices and len(chunk.choices) > 0:
            delta = chunk.choices[0].delta
            if hasattr(delta, 'content') and delta.content:
                print(delta.content, end="", flush=True)

    print("\n")


def exemplo_9_parametros_customizados():
    """Exemplo 9: Usando par√¢metros customizados"""
    print("=" * 60)
    print("üÜì EXEMPLO 9: Par√¢metros Customizados")
    print("=" * 60)
    print()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Escreva um slogan criativo para uma empresa de tecnologia"}
        ],
        temperature=0.9,  # Mais criativo (0.0 = conservador, 1.0 = criativo)
        max_tokens=100,   # Limita tamanho da resposta
        top_p=0.9,        # Nucleus sampling
    )

    print("üé® Resposta Criativa:")
    print(response.choices[0].message.content)
    print()
    print("‚öôÔ∏è Par√¢metros usados:")
    print("   ‚Ä¢ Temperature: 0.9 (mais criativo)")
    print("   ‚Ä¢ Max tokens: 100")
    print("   ‚Ä¢ Top P: 0.9")
    print()


def exemplo_10_multiplas_tarefas():
    """Exemplo 10: M√∫ltiplas tarefas em uma mensagem"""
    print("=" * 60)
    print("üÜì EXEMPLO 10: M√∫ltiplas Tarefas")
    print("=" * 60)
    print()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "user",
                "content": """
                Fa√ßa as seguintes tarefas:
                1. Traduza "Good morning" para portugu√™s
                2. Calcule 15% de 200
                3. D√™ uma dica de produtividade
                """
            }
        ]
    )

    print("üìù Respostas:")
    print(response.choices[0].message.content)
    print()


def main():
    """Executa todos os exemplos"""
    print("\n")
    print("üéâ EXEMPLOS PR√ÅTICOS - GLM-4.5-Flash (GRATUITO)")
    print("=" * 60)
    print("Este modelo √© 100% GRATUITO!")
    print("Use quantas vezes quiser (respeitando rate limits)")
    print("=" * 60)
    print("\n")

    try:
        # Executa cada exemplo
        exemplo_1_chat_simples()
        exemplo_2_traducao()
        exemplo_3_resumo()
        exemplo_4_geracao_codigo()
        exemplo_5_analise_dados()
        exemplo_6_criacao_conteudo()
        exemplo_7_conversacao_com_historico()
        exemplo_8_streaming()
        exemplo_9_parametros_customizados()
        exemplo_10_multiplas_tarefas()

        print("=" * 60)
        print("‚úÖ TODOS OS EXEMPLOS EXECUTADOS COM SUCESSO!")
        print("=" * 60)
        print()
        print("üí° DICAS:")
        print("   ‚Ä¢ Use temperature alta (0.7-1.0) para respostas criativas")
        print("   ‚Ä¢ Use temperature baixa (0.1-0.3) para respostas precisas")
        print("   ‚Ä¢ Use stream=True para respostas em tempo real")
        print("   ‚Ä¢ Mantenha hist√≥rico de mensagens para conversa√ß√£o")
        print("   ‚Ä¢ Limite: 2 requisi√ß√µes simult√¢neas")
        print()

    except Exception as e:
        print(f"\n‚ùå Erro: {e}\n")
        print("Verifique:")
        print("  ‚Ä¢ Arquivo .env com ZAI_API_KEY configurada")
        print("  ‚Ä¢ Conex√£o com internet")


if __name__ == "__main__":
    main()
