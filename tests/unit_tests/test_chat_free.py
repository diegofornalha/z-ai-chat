#!/usr/bin/env python3
"""
üÜì TESTE DO CHAT COM GLM-4.5-Flash (GRATUITO)

Testa todas as funcionalidades do client.chat.completions.create()
usando o modelo gratuito GLM-4.5-Flash
"""
from dotenv import load_dotenv
from zai import ZaiClient

# Carrega .env
load_dotenv()

# Modelo GRATUITO
MODEL = "glm-4.5-flash"


def test_chat_basico():
    """Teste 1: Chat b√°sico"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 1: Chat B√°sico")
    print("=" * 60)

    client = ZaiClient()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Ol√°! Como voc√™ est√°?"}
        ]
    )

    print(f"‚úÖ Status: Sucesso")
    print(f"üìù Resposta: {response.choices[0].message.content}")
    print(f"üìä Tokens: {response.usage.total_tokens}")
    print(f"üÜî ID: {response.id}")
    print(f"üè∑Ô∏è  Model: {response.model}")


def test_chat_com_parametros():
    """Teste 2: Chat com par√¢metros customizados"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 2: Chat com Par√¢metros Customizados")
    print("=" * 60)

    client = ZaiClient()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Escreva um slogan criativo para uma cafeteria"}
        ],
        temperature=0.9,  # Mais criativo
        top_p=0.95,
        max_tokens=50
    )

    print(f"‚úÖ Status: Sucesso")
    print(f"üìù Resposta: {response.choices[0].message.content}")
    print(f"‚öôÔ∏è  Temperature: 0.9")
    print(f"‚öôÔ∏è  Top P: 0.95")
    print(f"‚öôÔ∏è  Max Tokens: 50")


def test_chat_streaming():
    """Teste 3: Chat com streaming"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 3: Chat com Streaming (Tempo Real)")
    print("=" * 60)

    client = ZaiClient()

    print("üì° Streaming iniciado...")
    print("ü§ñ Resposta: ", end="", flush=True)

    stream = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Conte uma piada curta"}
        ],
        stream=True
    )

    full_response = ""
    for chunk in stream:
        if chunk.choices and len(chunk.choices) > 0:
            delta = chunk.choices[0].delta
            if hasattr(delta, 'content') and delta.content:
                print(delta.content, end="", flush=True)
                full_response += delta.content

    print("\n‚úÖ Streaming completado!")


def test_chat_conversacao():
    """Teste 4: Conversa√ß√£o com hist√≥rico"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 4: Conversa√ß√£o com Hist√≥rico")
    print("=" * 60)

    client = ZaiClient()

    # Conversa com m√∫ltiplas mensagens
    messages = [
        {"role": "system", "content": "Voc√™ √© um assistente √∫til e amig√°vel."},
        {"role": "user", "content": "Meu nome √© Maria e eu amo programa√ß√£o Python."}
    ]

    response1 = client.chat.completions.create(
        model=MODEL,
        messages=messages
    )

    print("üë§ Usu√°rio: Meu nome √© Maria e eu amo programa√ß√£o Python.")
    print(f"ü§ñ Assistente: {response1.choices[0].message.content}\n")

    # Adiciona resposta ao hist√≥rico
    messages.append({
        "role": "assistant",
        "content": response1.choices[0].message.content
    })

    # Pr√≥xima mensagem (testa mem√≥ria)
    messages.append({
        "role": "user",
        "content": "Qual √© o meu nome e o que eu gosto?"
    })

    response2 = client.chat.completions.create(
        model=MODEL,
        messages=messages
    )

    print("üë§ Usu√°rio: Qual √© o meu nome e o que eu gosto?")
    print(f"ü§ñ Assistente: {response2.choices[0].message.content}")
    print("\n‚úÖ Mem√≥ria funcionando corretamente!")


def test_chat_multi_turn():
    """Teste 5: Conversa√ß√£o de m√∫ltiplas rodadas"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 5: Conversa√ß√£o Multi-Turn")
    print("=" * 60)

    client = ZaiClient()

    messages = []

    # Rodada 1
    messages.append({"role": "user", "content": "Liste 3 linguagens de programa√ß√£o"})
    response1 = client.chat.completions.create(model=MODEL, messages=messages)
    print(f"üë§ Rodada 1: {messages[-1]['content']}")
    print(f"ü§ñ Resposta: {response1.choices[0].message.content}\n")
    messages.append({"role": "assistant", "content": response1.choices[0].message.content})

    # Rodada 2
    messages.append({"role": "user", "content": "Qual delas √© melhor para iniciantes?"})
    response2 = client.chat.completions.create(model=MODEL, messages=messages)
    print(f"üë§ Rodada 2: {messages[-1]['content']}")
    print(f"ü§ñ Resposta: {response2.choices[0].message.content}\n")
    messages.append({"role": "assistant", "content": response2.choices[0].message.content})

    # Rodada 3
    messages.append({"role": "user", "content": "Por qu√™?"})
    response3 = client.chat.completions.create(model=MODEL, messages=messages)
    print(f"üë§ Rodada 3: {messages[-1]['content']}")
    print(f"ü§ñ Resposta: {response3.choices[0].message.content}\n")

    print(f"‚úÖ Total de mensagens no hist√≥rico: {len(messages)}")


def test_chat_system_prompt():
    """Teste 6: Usando System Prompt"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 6: System Prompt (Define Personalidade)")
    print("=" * 60)

    client = ZaiClient()

    # Com system prompt
    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {
                "role": "system",
                "content": "Voc√™ √© um poeta que responde tudo em formato de haiku."
            },
            {"role": "user", "content": "O que √© intelig√™ncia artificial?"}
        ]
    )

    print("‚öôÔ∏è  System Prompt: 'Voc√™ √© um poeta que responde tudo em formato de haiku'")
    print(f"üìù Resposta em haiku:")
    print(response.choices[0].message.content)
    print("\n‚úÖ System prompt aplicado com sucesso!")


def test_chat_diferentes_temperaturas():
    """Teste 7: Comparando diferentes temperaturas"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 7: Comparando Temperaturas")
    print("=" * 60)

    client = ZaiClient()
    prompt = "Complete a frase: O futuro da tecnologia √©"

    # Temperatura baixa (mais conservador)
    response_low = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.1,
        max_tokens=30
    )

    print(f"üå°Ô∏è  Temperature 0.1 (conservador):")
    print(f"   {response_low.choices[0].message.content}\n")

    # Temperatura alta (mais criativo)
    response_high = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.9,
        max_tokens=30
    )

    print(f"üå°Ô∏è  Temperature 0.9 (criativo):")
    print(f"   {response_high.choices[0].message.content}\n")

    print("‚úÖ Diferentes temperaturas testadas!")


def test_chat_stop_sequences():
    """Teste 8: Stop Sequences"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 8: Stop Sequences")
    print("=" * 60)

    client = ZaiClient()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Liste 10 frutas, uma por linha"}
        ],
        stop=["\n5."],  # Para depois da 4¬™ fruta
        max_tokens=200
    )

    print("‚õî Stop sequence: '\\n5.' (para na 4¬™ fruta)")
    print(f"üìù Resposta:")
    print(response.choices[0].message.content)
    print("\n‚úÖ Stop sequence aplicada!")


def test_chat_seed_reproducibilidade():
    """Teste 9: Seed para reprodutibilidade"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 9: Seed (Respostas Reproduz√≠veis)")
    print("=" * 60)

    client = ZaiClient()
    prompt = "Escreva um n√∫mero aleat√≥rio"

    # Primeira chamada com seed
    response1 = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        seed=42,
        temperature=0.7
    )

    # Segunda chamada com mesmo seed
    response2 = client.chat.completions.create(
        model=MODEL,
        messages=[{"role": "user", "content": prompt}],
        seed=42,
        temperature=0.7
    )

    print(f"üé≤ Seed: 42")
    print(f"üìù Resposta 1: {response1.choices[0].message.content}")
    print(f"üìù Resposta 2: {response2.choices[0].message.content}")

    if response1.choices[0].message.content == response2.choices[0].message.content:
        print("\n‚úÖ Respostas id√™nticas! Seed funcionando!")
    else:
        print("\n‚ö†Ô∏è  Respostas diferentes (seed pode n√£o garantir 100% de reprodutibilidade)")


def test_chat_response_metadata():
    """Teste 10: Verificando Metadata da Resposta"""
    print("\n" + "=" * 60)
    print("üß™ TESTE 10: Metadata da Resposta")
    print("=" * 60)

    client = ZaiClient()

    response = client.chat.completions.create(
        model=MODEL,
        messages=[
            {"role": "user", "content": "Ol√°"}
        ]
    )

    print("üìã Campos dispon√≠veis na resposta:")
    print(f"   ‚Ä¢ id: {response.id}")
    print(f"   ‚Ä¢ model: {response.model}")
    print(f"   ‚Ä¢ created: {response.created}")

    if hasattr(response, 'usage'):
        print(f"   ‚Ä¢ usage.prompt_tokens: {response.usage.prompt_tokens}")
        print(f"   ‚Ä¢ usage.completion_tokens: {response.usage.completion_tokens}")
        print(f"   ‚Ä¢ usage.total_tokens: {response.usage.total_tokens}")

    if response.choices and len(response.choices) > 0:
        choice = response.choices[0]
        print(f"   ‚Ä¢ choices[0].message.role: {choice.message.role}")
        print(f"   ‚Ä¢ choices[0].message.content: {choice.message.content[:50]}...")
        if hasattr(choice, 'finish_reason'):
            print(f"   ‚Ä¢ choices[0].finish_reason: {choice.finish_reason}")

    print("\n‚úÖ Metadata completa!")


def main():
    """Executa todos os testes"""
    print("\n")
    print("üéØ TESTES DO CHAT API COM GLM-4.5-Flash (GRATUITO)")
    print("=" * 60)
    print("Testando todas as funcionalidades de client.chat.completions.create()")
    print("=" * 60)

    try:
        test_chat_basico()
        test_chat_com_parametros()
        test_chat_streaming()
        test_chat_conversacao()
        test_chat_multi_turn()
        test_chat_system_prompt()
        test_chat_diferentes_temperaturas()
        test_chat_stop_sequences()
        test_chat_seed_reproducibilidade()
        test_chat_response_metadata()

        print("\n" + "=" * 60)
        print("‚úÖ TODOS OS TESTES EXECUTADOS COM SUCESSO!")
        print("=" * 60)
        print()
        print("üìä RESUMO:")
        print("   ‚Ä¢ 10 testes executados")
        print("   ‚Ä¢ Modelo: GLM-4.5-Flash (GRATUITO)")
        print("   ‚Ä¢ Custo total: $0.00")
        print()
        print("üí° Voc√™ pode usar este modelo ILIMITADAMENTE!")
        print("   (Respeitando o limite de 2 requisi√ß√µes simult√¢neas)")
        print()

    except Exception as e:
        print(f"\n‚ùå Erro: {e}\n")


if __name__ == "__main__":
    main()
